{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9260442a7fe0355e\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-9260442a7fe0355e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42370a9fea9a4366bdf69155078b02e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A classification problem with 5 classes: ['anger', 'fear', 'happiness', 'neutral', 'sadness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-9260442a7fe0355e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-daff82a994f3b041.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-9260442a7fe0355e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-84ae9b7c5010f384.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-9260442a7fe0355e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-a539164cb1745db6.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-9260442a7fe0355e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-287f889749556e7d.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-9260442a7fe0355e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-1529970b32788e16.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-9260442a7fe0355e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-98069b278ed3cff3.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-9260442a7fe0355e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-f7edbe7e21107fc6.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-9260442a7fe0355e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-a0afb083dd200ba7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target sampling rate: 16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/hubert-large-ls960-ft were not used when initializing Wav2Vec2ForSpeechClassification: ['hubert.encoder.layers.10.final_layer_norm.bias', 'hubert.encoder.layers.6.attention.q_proj.weight', 'hubert.encoder.layers.9.final_layer_norm.bias', 'hubert.encoder.layers.0.layer_norm.bias', 'hubert.encoder.layers.10.attention.q_proj.weight', 'hubert.encoder.layers.3.feed_forward.intermediate_dense.bias', 'hubert.encoder.layers.12.attention.k_proj.weight', 'hubert.encoder.layers.17.attention.v_proj.bias', 'hubert.encoder.layers.3.attention.v_proj.weight', 'hubert.encoder.layers.22.attention.q_proj.weight', 'hubert.encoder.layers.0.attention.q_proj.bias', 'hubert.encoder.layers.11.feed_forward.output_dense.bias', 'hubert.encoder.layers.11.feed_forward.intermediate_dense.weight', 'hubert.encoder.layers.13.attention.k_proj.weight', 'hubert.encoder.layers.8.attention.v_proj.bias', 'hubert.encoder.layers.7.attention.out_proj.bias', 'hubert.encoder.layers.23.final_layer_norm.bias', 'hubert.encoder.layers.2.attention.out_proj.weight', 'hubert.encoder.layers.7.feed_forward.output_dense.weight', 'hubert.encoder.layers.1.feed_forward.intermediate_dense.bias', 'hubert.encoder.layers.8.attention.out_proj.weight', 'hubert.feature_extractor.conv_layers.1.conv.weight', 'hubert.encoder.layers.17.attention.q_proj.bias', 'hubert.encoder.layers.19.attention.out_proj.bias', 'hubert.encoder.layers.22.attention.k_proj.bias', 'hubert.encoder.layers.19.attention.k_proj.weight', 'hubert.encoder.layers.4.attention.q_proj.weight', 'hubert.encoder.layers.6.feed_forward.output_dense.weight', 'hubert.encoder.layers.0.attention.k_proj.bias', 'hubert.encoder.layers.18.feed_forward.intermediate_dense.bias', 'hubert.encoder.pos_conv_embed.conv.weight_v', 'hubert.encoder.layers.18.attention.v_proj.bias', 'hubert.encoder.layers.12.final_layer_norm.bias', 'hubert.encoder.layers.21.attention.v_proj.bias', 'hubert.encoder.layers.20.layer_norm.bias', 'hubert.encoder.layers.14.layer_norm.weight', 'hubert.encoder.layers.0.attention.out_proj.bias', 'hubert.encoder.layers.13.attention.k_proj.bias', 'hubert.encoder.layers.9.feed_forward.output_dense.bias', 'hubert.encoder.layers.5.attention.q_proj.bias', 'hubert.feature_extractor.conv_layers.4.conv.bias', 'hubert.encoder.layers.6.attention.q_proj.bias', 'hubert.encoder.layers.21.layer_norm.weight', 'hubert.encoder.layers.16.attention.k_proj.bias', 'hubert.encoder.layers.1.attention.out_proj.weight', 'hubert.encoder.layers.10.feed_forward.intermediate_dense.weight', 'hubert.feature_extractor.conv_layers.3.conv.bias', 'hubert.encoder.layers.21.layer_norm.bias', 'hubert.encoder.layers.20.final_layer_norm.weight', 'lm_head.bias', 'hubert.feature_extractor.conv_layers.5.conv.bias', 'hubert.encoder.layers.9.feed_forward.output_dense.weight', 'hubert.encoder.layers.5.attention.q_proj.weight', 'hubert.encoder.layers.4.attention.k_proj.bias', 'hubert.encoder.layers.23.attention.k_proj.bias', 'hubert.encoder.layers.22.final_layer_norm.weight', 'hubert.encoder.layers.17.attention.k_proj.bias', 'hubert.encoder.layers.2.attention.k_proj.bias', 'hubert.encoder.layers.12.attention.out_proj.bias', 'hubert.encoder.layers.18.layer_norm.bias', 'hubert.encoder.layers.9.attention.v_proj.bias', 'hubert.encoder.layers.14.layer_norm.bias', 'hubert.encoder.layers.15.layer_norm.weight', 'hubert.encoder.layers.10.attention.out_proj.bias', 'hubert.feature_extractor.conv_layers.5.layer_norm.bias', 'hubert.encoder.layers.21.attention.k_proj.weight', 'hubert.encoder.layers.2.attention.out_proj.bias', 'hubert.encoder.layers.5.final_layer_norm.weight', 'hubert.encoder.layers.5.attention.k_proj.weight', 'hubert.encoder.layers.16.attention.q_proj.bias', 'hubert.encoder.layers.6.final_layer_norm.bias', 'hubert.encoder.layers.17.feed_forward.output_dense.bias', 'hubert.encoder.layers.21.attention.q_proj.bias', 'hubert.encoder.layer_norm.bias', 'hubert.encoder.layers.3.attention.k_proj.weight', 'hubert.encoder.layers.15.attention.v_proj.weight', 'hubert.encoder.layers.13.layer_norm.bias', 'hubert.feature_projection.layer_norm.weight', 'hubert.encoder.layers.3.final_layer_norm.bias', 'hubert.encoder.layers.4.feed_forward.output_dense.bias', 'hubert.encoder.layers.3.attention.k_proj.bias', 'hubert.encoder.layers.8.attention.q_proj.bias', 'hubert.encoder.layers.11.attention.v_proj.weight', 'hubert.encoder.layers.15.attention.out_proj.bias', 'hubert.encoder.layers.15.attention.q_proj.weight', 'hubert.encoder.layers.19.feed_forward.output_dense.weight', 'hubert.feature_extractor.conv_layers.1.layer_norm.weight', 'hubert.encoder.layers.4.attention.v_proj.bias', 'hubert.encoder.layers.8.attention.q_proj.weight', 'hubert.encoder.layers.11.attention.q_proj.weight', 'hubert.encoder.layers.20.attention.k_proj.bias', 'hubert.encoder.pos_conv_embed.conv.bias', 'hubert.encoder.layers.10.feed_forward.output_dense.weight', 'hubert.encoder.layers.6.attention.k_proj.bias', 'hubert.encoder.layers.15.feed_forward.output_dense.bias', 'hubert.encoder.layers.10.attention.v_proj.weight', 'hubert.encoder.layers.13.attention.q_proj.bias', 'hubert.feature_extractor.conv_layers.2.conv.bias', 'hubert.encoder.layers.0.feed_forward.intermediate_dense.bias', 'hubert.feature_extractor.conv_layers.2.conv.weight', 'hubert.encoder.layers.10.attention.k_proj.weight', 'hubert.encoder.layers.11.attention.k_proj.weight', 'hubert.encoder.layers.7.attention.v_proj.bias', 'hubert.encoder.layers.16.attention.v_proj.weight', 'hubert.encoder.layers.13.feed_forward.output_dense.bias', 'hubert.feature_extractor.conv_layers.2.layer_norm.weight', 'hubert.encoder.layers.7.layer_norm.bias', 'hubert.encoder.layers.11.attention.out_proj.bias', 'hubert.encoder.layers.12.attention.v_proj.weight', 'hubert.encoder.layers.16.layer_norm.weight', 'hubert.encoder.layers.14.attention.out_proj.bias', 'hubert.encoder.layers.4.feed_forward.intermediate_dense.bias', 'hubert.encoder.layers.19.attention.out_proj.weight', 'hubert.encoder.layers.4.attention.q_proj.bias', 'hubert.encoder.layers.9.attention.v_proj.weight', 'hubert.encoder.layers.21.attention.k_proj.bias', 'hubert.encoder.layers.1.attention.v_proj.bias', 'hubert.encoder.layers.6.feed_forward.intermediate_dense.weight', 'hubert.encoder.layers.1.attention.k_proj.weight', 'hubert.encoder.layers.22.attention.k_proj.weight', 'hubert.feature_extractor.conv_layers.3.layer_norm.weight', 'hubert.encoder.layers.9.attention.q_proj.bias', 'hubert.encoder.layers.9.feed_forward.intermediate_dense.weight', 'hubert.encoder.layers.19.attention.q_proj.weight', 'hubert.encoder.layers.23.attention.v_proj.weight', 'hubert.encoder.layers.23.attention.k_proj.weight', 'hubert.encoder.layers.1.attention.k_proj.bias', 'hubert.encoder.layers.8.attention.v_proj.weight', 'hubert.encoder.layers.2.attention.v_proj.bias', 'hubert.encoder.layers.11.feed_forward.intermediate_dense.bias', 'hubert.encoder.layers.3.feed_forward.intermediate_dense.weight', 'hubert.encoder.layers.10.attention.q_proj.bias', 'hubert.encoder.layers.5.feed_forward.output_dense.weight', 'hubert.encoder.layers.16.final_layer_norm.weight', 'hubert.encoder.layers.18.attention.k_proj.bias', 'hubert.encoder.layers.16.layer_norm.bias', 'hubert.encoder.layers.13.feed_forward.intermediate_dense.weight', 'hubert.encoder.layers.18.attention.v_proj.weight', 'hubert.encoder.layers.1.final_layer_norm.weight', 'hubert.encoder.layers.1.final_layer_norm.bias', 'hubert.encoder.layers.5.attention.k_proj.bias', 'hubert.encoder.layers.7.final_layer_norm.weight', 'hubert.feature_projection.layer_norm.bias', 'hubert.encoder.layers.7.feed_forward.intermediate_dense.weight', 'hubert.encoder.layers.7.attention.q_proj.weight', 'hubert.encoder.layers.19.attention.q_proj.bias', 'hubert.encoder.layers.11.final_layer_norm.weight', 'hubert.encoder.layers.17.layer_norm.weight', 'hubert.encoder.layers.16.attention.out_proj.weight', 'hubert.encoder.layers.2.attention.q_proj.weight', 'hubert.encoder.layers.23.feed_forward.output_dense.weight', 'hubert.encoder.layers.8.attention.out_proj.bias', 'hubert.encoder.layers.0.feed_forward.output_dense.weight', 'hubert.encoder.layers.23.attention.out_proj.weight', 'hubert.feature_projection.projection.weight', 'hubert.encoder.layers.19.feed_forward.output_dense.bias', 'hubert.encoder.layers.20.attention.q_proj.weight', 'hubert.encoder.layers.17.feed_forward.intermediate_dense.weight', 'hubert.encoder.layers.4.feed_forward.output_dense.weight', 'hubert.encoder.layers.6.attention.v_proj.bias', 'hubert.encoder.layers.20.attention.v_proj.weight', 'hubert.encoder.layers.21.attention.out_proj.weight', 'hubert.encoder.layers.1.layer_norm.bias', 'hubert.encoder.layers.2.feed_forward.output_dense.bias', 'hubert.encoder.layers.19.attention.k_proj.bias', 'hubert.encoder.layers.11.layer_norm.bias', 'hubert.encoder.layers.8.attention.k_proj.weight', 'hubert.encoder.layers.16.attention.q_proj.weight', 'hubert.encoder.layers.10.attention.out_proj.weight', 'hubert.feature_extractor.conv_layers.1.conv.bias', 'hubert.encoder.layers.5.feed_forward.output_dense.bias', 'hubert.encoder.layers.19.feed_forward.intermediate_dense.weight', 'hubert.encoder.layers.5.layer_norm.weight', 'hubert.encoder.layers.16.feed_forward.output_dense.bias', 'hubert.encoder.layers.18.final_layer_norm.weight', 'hubert.feature_extractor.conv_layers.5.layer_norm.weight', 'hubert.encoder.layers.14.attention.q_proj.bias', 'hubert.encoder.layers.5.final_layer_norm.bias', 'hubert.encoder.layers.18.feed_forward.output_dense.weight', 'hubert.encoder.layers.3.attention.out_proj.bias', 'hubert.encoder.layers.1.attention.q_proj.bias', 'hubert.encoder.layers.17.feed_forward.intermediate_dense.bias', 'hubert.encoder.layers.0.attention.v_proj.weight', 'hubert.encoder.layers.5.feed_forward.intermediate_dense.weight', 'hubert.encoder.layers.16.feed_forward.intermediate_dense.bias', 'hubert.encoder.layers.12.layer_norm.weight', 'hubert.encoder.layers.12.feed_forward.intermediate_dense.weight', 'hubert.encoder.layers.10.feed_forward.output_dense.bias', 'hubert.encoder.layers.6.attention.out_proj.bias', 'hubert.encoder.layers.3.attention.v_proj.bias', 'hubert.encoder.layers.21.attention.v_proj.weight', 'hubert.encoder.layers.2.layer_norm.bias', 'hubert.encoder.layers.15.final_layer_norm.bias', 'hubert.encoder.layers.1.attention.v_proj.weight', 'hubert.feature_extractor.conv_layers.3.conv.weight', 'hubert.encoder.layers.19.layer_norm.bias', 'hubert.encoder.layers.17.final_layer_norm.bias', 'hubert.encoder.layers.17.attention.out_proj.weight', 'hubert.encoder.layers.19.final_layer_norm.bias', 'hubert.encoder.layers.23.feed_forward.intermediate_dense.bias', 'hubert.encoder.layers.7.layer_norm.weight', 'hubert.feature_projection.projection.bias', 'hubert.encoder.layers.12.attention.q_proj.weight', 'hubert.feature_extractor.conv_layers.6.layer_norm.weight', 'hubert.encoder.layers.4.attention.k_proj.weight', 'hubert.encoder.layers.3.layer_norm.bias', 'hubert.encoder.layers.17.attention.v_proj.weight', 'hubert.encoder.layers.10.layer_norm.bias', 'hubert.encoder.layers.10.final_layer_norm.weight', 'hubert.encoder.layers.4.layer_norm.bias', 'hubert.encoder.layers.14.feed_forward.intermediate_dense.bias', 'hubert.encoder.layers.12.attention.v_proj.bias', 'hubert.encoder.layers.16.attention.out_proj.bias', 'hubert.encoder.layers.14.attention.k_proj.weight', 'hubert.feature_extractor.conv_layers.4.layer_norm.weight', 'hubert.encoder.layers.22.feed_forward.intermediate_dense.weight', 'hubert.encoder.layers.12.attention.q_proj.bias', 'hubert.encoder.layers.8.layer_norm.weight', 'hubert.encoder.layers.21.attention.q_proj.weight', 'hubert.encoder.layers.7.attention.k_proj.bias', 'hubert.encoder.layers.22.attention.q_proj.bias', 'hubert.encoder.layers.3.attention.out_proj.weight', 'hubert.encoder.layers.23.attention.v_proj.bias', 'hubert.encoder.layers.23.final_layer_norm.weight', 'hubert.encoder.layers.20.attention.k_proj.weight', 'hubert.encoder.layers.2.attention.v_proj.weight', 'hubert.encoder.layers.10.layer_norm.weight', 'hubert.feature_extractor.conv_layers.0.layer_norm.bias', 'hubert.encoder.layers.4.layer_norm.weight', 'hubert.encoder.layers.9.final_layer_norm.weight', 'hubert.encoder.layers.7.attention.k_proj.weight', 'hubert.encoder.layers.17.attention.q_proj.weight', 'hubert.encoder.layers.2.feed_forward.intermediate_dense.bias', 'hubert.encoder.layers.15.attention.v_proj.bias', 'hubert.encoder.layers.8.feed_forward.intermediate_dense.weight', 'hubert.encoder.layers.15.attention.q_proj.bias', 'hubert.encoder.layers.2.final_layer_norm.bias', 'hubert.encoder.layers.22.feed_forward.output_dense.bias', 'hubert.encoder.layers.14.attention.q_proj.weight', 'hubert.encoder.layers.14.feed_forward.output_dense.bias', 'hubert.encoder.layers.7.final_layer_norm.bias', 'hubert.encoder.layers.22.attention.v_proj.weight', 'hubert.encoder.layers.8.final_layer_norm.weight', 'hubert.encoder.layers.1.feed_forward.output_dense.weight', 'hubert.encoder.layers.16.feed_forward.intermediate_dense.weight', 'hubert.encoder.layers.4.final_layer_norm.weight', 'hubert.encoder.layers.14.attention.k_proj.bias', 'hubert.encoder.layers.21.attention.out_proj.bias', 'hubert.encoder.layers.13.attention.q_proj.weight', 'hubert.encoder.layers.2.layer_norm.weight', 'hubert.encoder.layers.20.feed_forward.output_dense.bias', 'hubert.encoder.layers.23.attention.out_proj.bias', 'hubert.encoder.layers.9.attention.k_proj.weight', 'hubert.encoder.layers.11.attention.v_proj.bias', 'hubert.encoder.layers.5.attention.v_proj.weight', 'hubert.encoder.layers.9.feed_forward.intermediate_dense.bias', 'hubert.feature_extractor.conv_layers.6.conv.bias', 'hubert.encoder.layers.10.attention.v_proj.bias', 'hubert.encoder.layers.5.feed_forward.intermediate_dense.bias', 'hubert.encoder.layers.6.layer_norm.bias', 'hubert.encoder.layers.2.attention.q_proj.bias', 'hubert.encoder.layers.0.attention.k_proj.weight', 'hubert.encoder.layers.15.feed_forward.intermediate_dense.weight', 'hubert.feature_extractor.conv_layers.5.conv.weight', 'hubert.encoder.layers.13.attention.v_proj.weight', 'hubert.encoder.layers.21.feed_forward.intermediate_dense.bias', 'hubert.encoder.layers.17.attention.out_proj.bias', 'hubert.encoder.layers.18.layer_norm.weight', 'hubert.encoder.layers.1.feed_forward.intermediate_dense.weight', 'hubert.encoder.layers.8.feed_forward.intermediate_dense.bias', 'hubert.encoder.layers.12.layer_norm.bias', 'hubert.encoder.layers.6.feed_forward.intermediate_dense.bias', 'hubert.encoder.layers.17.attention.k_proj.weight', 'hubert.feature_extractor.conv_layers.0.layer_norm.weight', 'hubert.encoder.layers.23.feed_forward.output_dense.bias', 'hubert.encoder.layers.13.feed_forward.intermediate_dense.bias', 'hubert.encoder.layers.4.feed_forward.intermediate_dense.weight', 'hubert.encoder.layers.12.attention.k_proj.bias', 'hubert.encoder.layers.1.attention.q_proj.weight', 'hubert.encoder.layers.18.attention.k_proj.weight', 'hubert.encoder.layers.0.final_layer_norm.weight', 'hubert.encoder.layers.9.attention.out_proj.weight', 'hubert.encoder.layers.12.attention.out_proj.weight', 'hubert.encoder.layers.6.layer_norm.weight', 'hubert.encoder.layers.8.feed_forward.output_dense.weight', 'hubert.encoder.layers.22.layer_norm.bias', 'hubert.encoder.layers.19.attention.v_proj.bias', 'hubert.encoder.layers.8.layer_norm.bias', 'hubert.encoder.layers.13.attention.out_proj.bias', 'hubert.encoder.layers.18.feed_forward.output_dense.bias', 'hubert.encoder.layers.20.attention.out_proj.bias', 'hubert.encoder.layers.8.feed_forward.output_dense.bias', 'hubert.encoder.layers.14.final_layer_norm.bias', 'hubert.encoder.layers.15.feed_forward.intermediate_dense.bias', 'hubert.encoder.layers.7.attention.v_proj.weight', 'hubert.encoder.layers.10.attention.k_proj.bias', 'hubert.encoder.layers.13.attention.v_proj.bias', 'hubert.encoder.layers.0.feed_forward.output_dense.bias', 'hubert.encoder.layers.9.layer_norm.weight', 'hubert.encoder.layers.9.attention.out_proj.bias', 'hubert.encoder.layers.4.attention.out_proj.weight', 'lm_head.weight', 'hubert.encoder.layers.19.attention.v_proj.weight', 'hubert.encoder.layers.20.attention.v_proj.bias', 'hubert.encoder.layers.13.layer_norm.weight', 'hubert.encoder.layers.22.layer_norm.weight', 'hubert.encoder.layers.3.final_layer_norm.weight', 'hubert.encoder.layers.1.feed_forward.output_dense.bias', 'hubert.encoder.layers.2.attention.k_proj.weight', 'hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.layers.2.feed_forward.output_dense.weight', 'hubert.encoder.layers.18.attention.q_proj.bias', 'hubert.encoder.layers.22.attention.out_proj.weight', 'hubert.encoder.layers.20.feed_forward.intermediate_dense.bias', 'hubert.encoder.layer_norm.weight', 'hubert.encoder.layers.6.feed_forward.output_dense.bias', 'hubert.encoder.layers.21.final_layer_norm.weight', 'hubert.encoder.layers.3.attention.q_proj.bias', 'hubert.encoder.layers.11.final_layer_norm.bias', 'hubert.feature_extractor.conv_layers.6.layer_norm.bias', 'hubert.encoder.layers.14.attention.v_proj.weight', 'hubert.encoder.layers.20.feed_forward.intermediate_dense.weight', 'hubert.encoder.layers.23.feed_forward.intermediate_dense.weight', 'hubert.encoder.layers.3.attention.q_proj.weight', 'hubert.encoder.layers.16.attention.v_proj.bias', 'hubert.encoder.layers.11.attention.q_proj.bias', 'hubert.encoder.layers.16.final_layer_norm.bias', 'hubert.feature_extractor.conv_layers.6.conv.weight', 'hubert.encoder.layers.7.feed_forward.intermediate_dense.bias', 'hubert.encoder.layers.9.attention.q_proj.weight', 'hubert.encoder.layers.15.final_layer_norm.weight', 'hubert.encoder.layers.19.feed_forward.intermediate_dense.bias', 'hubert.feature_extractor.conv_layers.0.conv.weight', 'hubert.encoder.layers.15.attention.out_proj.weight', 'hubert.encoder.layers.5.layer_norm.bias', 'hubert.masked_spec_embed', 'hubert.encoder.layers.22.final_layer_norm.bias', 'hubert.encoder.layers.12.feed_forward.output_dense.bias', 'hubert.encoder.layers.20.attention.out_proj.weight', 'hubert.encoder.layers.0.attention.v_proj.bias', 'hubert.encoder.layers.1.attention.out_proj.bias', 'hubert.encoder.layers.11.attention.k_proj.bias', 'hubert.encoder.layers.4.attention.out_proj.bias', 'hubert.encoder.layers.11.layer_norm.weight', 'hubert.encoder.layers.13.final_layer_norm.weight', 'hubert.feature_extractor.conv_layers.2.layer_norm.bias', 'hubert.encoder.layers.0.final_layer_norm.bias', 'hubert.encoder.layers.23.attention.q_proj.bias', 'hubert.encoder.layers.4.attention.v_proj.weight', 'hubert.encoder.layers.15.layer_norm.bias', 'hubert.encoder.layers.22.attention.v_proj.bias', 'hubert.feature_extractor.conv_layers.0.conv.bias', 'hubert.encoder.layers.23.layer_norm.weight', 'hubert.encoder.layers.13.feed_forward.output_dense.weight', 'hubert.encoder.layers.16.feed_forward.output_dense.weight', 'hubert.encoder.layers.6.attention.k_proj.weight', 'hubert.encoder.layers.21.final_layer_norm.bias', 'hubert.encoder.layers.18.final_layer_norm.bias', 'hubert.encoder.layers.7.attention.out_proj.weight', 'hubert.encoder.layers.9.attention.k_proj.bias', 'hubert.encoder.layers.1.layer_norm.weight', 'hubert.encoder.layers.0.attention.out_proj.weight', 'hubert.encoder.layers.8.final_layer_norm.bias', 'hubert.encoder.layers.5.attention.v_proj.bias', 'hubert.feature_extractor.conv_layers.1.layer_norm.bias', 'hubert.encoder.layers.7.attention.q_proj.bias', 'hubert.encoder.layers.12.feed_forward.intermediate_dense.bias', 'hubert.encoder.layers.11.attention.out_proj.weight', 'hubert.feature_extractor.conv_layers.3.layer_norm.bias', 'hubert.encoder.layers.22.feed_forward.intermediate_dense.bias', 'hubert.encoder.layers.21.feed_forward.output_dense.weight', 'hubert.encoder.layers.23.attention.q_proj.weight', 'hubert.encoder.layers.17.layer_norm.bias', 'hubert.encoder.layers.21.feed_forward.output_dense.bias', 'hubert.encoder.layers.18.attention.q_proj.weight', 'hubert.encoder.layers.15.attention.k_proj.bias', 'hubert.encoder.layers.10.feed_forward.intermediate_dense.bias', 'hubert.encoder.layers.6.attention.v_proj.weight', 'hubert.encoder.layers.3.feed_forward.output_dense.bias', 'hubert.feature_extractor.conv_layers.4.conv.weight', 'hubert.encoder.layers.9.layer_norm.bias', 'hubert.encoder.layers.8.attention.k_proj.bias', 'hubert.encoder.layers.16.attention.k_proj.weight', 'hubert.encoder.layers.18.attention.out_proj.bias', 'hubert.encoder.layers.18.feed_forward.intermediate_dense.weight', 'hubert.encoder.layers.22.attention.out_proj.bias', 'hubert.encoder.layers.12.feed_forward.output_dense.weight', 'hubert.feature_extractor.conv_layers.4.layer_norm.bias', 'hubert.encoder.layers.19.final_layer_norm.weight', 'hubert.encoder.layers.6.final_layer_norm.weight', 'hubert.encoder.layers.19.layer_norm.weight', 'hubert.encoder.layers.12.final_layer_norm.weight', 'hubert.encoder.layers.17.feed_forward.output_dense.weight', 'hubert.encoder.layers.2.final_layer_norm.weight', 'hubert.encoder.layers.3.feed_forward.output_dense.weight', 'hubert.encoder.layers.0.attention.q_proj.weight', 'hubert.encoder.layers.13.final_layer_norm.bias', 'hubert.encoder.layers.17.final_layer_norm.weight', 'hubert.encoder.layers.3.layer_norm.weight', 'hubert.encoder.layers.4.final_layer_norm.bias', 'hubert.encoder.layers.14.final_layer_norm.weight', 'hubert.encoder.layers.18.attention.out_proj.weight', 'hubert.encoder.layers.20.layer_norm.weight', 'hubert.encoder.layers.14.attention.out_proj.weight', 'hubert.encoder.layers.6.attention.out_proj.weight', 'hubert.encoder.layers.14.attention.v_proj.bias', 'hubert.encoder.layers.23.layer_norm.bias', 'hubert.encoder.layers.15.feed_forward.output_dense.weight', 'hubert.encoder.layers.20.attention.q_proj.bias', 'hubert.encoder.layers.0.feed_forward.intermediate_dense.weight', 'hubert.encoder.layers.13.attention.out_proj.weight', 'hubert.encoder.layers.0.layer_norm.weight', 'hubert.encoder.layers.14.feed_forward.output_dense.weight', 'hubert.encoder.layers.5.attention.out_proj.bias', 'hubert.encoder.layers.22.feed_forward.output_dense.weight', 'hubert.encoder.layers.20.final_layer_norm.bias', 'hubert.encoder.layers.15.attention.k_proj.weight', 'hubert.encoder.layers.7.feed_forward.output_dense.bias', 'hubert.encoder.layers.20.feed_forward.output_dense.weight', 'hubert.encoder.layers.11.feed_forward.output_dense.weight', 'hubert.encoder.layers.2.feed_forward.intermediate_dense.weight', 'hubert.encoder.layers.5.attention.out_proj.weight', 'hubert.encoder.layers.14.feed_forward.intermediate_dense.weight', 'hubert.encoder.layers.21.feed_forward.intermediate_dense.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['encoder.layers.23.layer_norm.weight', 'encoder.layers.17.attention.out_proj.weight', 'encoder.layers.21.attention.q_proj.weight', 'encoder.layers.19.layer_norm.weight', 'encoder.layers.13.feed_forward.intermediate_dense.weight', 'encoder.layers.3.feed_forward.output_dense.bias', 'encoder.layers.6.attention.k_proj.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.9.attention.q_proj.bias', 'encoder.layers.21.final_layer_norm.weight', 'feature_extractor.conv_layers.6.layer_norm.bias', 'encoder.layers.5.feed_forward.output_dense.weight', 'encoder.layers.17.layer_norm.weight', 'encoder.layers.3.attention.k_proj.bias', 'encoder.layers.19.attention.k_proj.bias', 'encoder.layers.6.feed_forward.intermediate_dense.weight', 'encoder.layers.13.attention.q_proj.weight', 'encoder.layers.8.attention.v_proj.weight', 'encoder.layers.23.final_layer_norm.weight', 'feature_extractor.conv_layers.6.conv.bias', 'encoder.layers.9.attention.v_proj.bias', 'encoder.layers.18.attention.k_proj.bias', 'encoder.layers.3.attention.out_proj.weight', 'encoder.layers.16.attention.k_proj.bias', 'encoder.layers.9.layer_norm.weight', 'encoder.layers.6.feed_forward.intermediate_dense.bias', 'feature_extractor.conv_layers.3.conv.bias', 'encoder.layers.17.attention.q_proj.bias', 'encoder.layers.6.layer_norm.weight', 'encoder.layers.23.attention.v_proj.bias', 'encoder.layers.14.attention.out_proj.bias', 'feature_extractor.conv_layers.0.layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.9.feed_forward.intermediate_dense.bias', 'encoder.layers.7.attention.v_proj.bias', 'encoder.layers.16.layer_norm.bias', 'encoder.layers.20.attention.out_proj.bias', 'encoder.layers.21.layer_norm.weight', 'encoder.layers.7.feed_forward.intermediate_dense.bias', 'encoder.layers.19.feed_forward.output_dense.bias', 'feature_extractor.conv_layers.2.conv.bias', 'encoder.layers.0.feed_forward.output_dense.weight', 'encoder.layers.4.attention.k_proj.bias', 'feature_extractor.conv_layers.0.layer_norm.bias', 'encoder.layers.11.attention.q_proj.bias', 'encoder.layers.9.feed_forward.output_dense.weight', 'encoder.layers.18.final_layer_norm.bias', 'encoder.layers.14.final_layer_norm.weight', 'encoder.layers.1.attention.out_proj.bias', 'feature_projection.layer_norm.bias', 'encoder.layers.19.layer_norm.bias', 'encoder.layers.12.attention.k_proj.bias', 'encoder.layers.8.layer_norm.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.attention.v_proj.bias', 'encoder.layers.22.layer_norm.weight', 'encoder.layers.2.attention.out_proj.bias', 'encoder.layers.17.feed_forward.output_dense.weight', 'encoder.layers.18.final_layer_norm.weight', 'feature_projection.layer_norm.weight', 'encoder.layers.17.feed_forward.output_dense.bias', 'encoder.layers.20.feed_forward.intermediate_dense.weight', 'encoder.layers.11.feed_forward.output_dense.weight', 'encoder.layers.9.attention.k_proj.bias', 'encoder.layers.12.feed_forward.intermediate_dense.bias', 'encoder.layers.9.feed_forward.intermediate_dense.weight', 'encoder.layers.0.feed_forward.output_dense.bias', 'encoder.layers.15.attention.v_proj.bias', 'encoder.layers.15.feed_forward.output_dense.bias', 'classifier.out_proj.bias', 'encoder.layers.14.attention.out_proj.weight', 'encoder.layers.19.feed_forward.intermediate_dense.bias', 'encoder.layers.19.final_layer_norm.weight', 'encoder.layers.11.layer_norm.weight', 'encoder.layers.20.attention.out_proj.weight', 'encoder.layers.16.attention.out_proj.weight', 'encoder.layers.11.attention.v_proj.weight', 'encoder.layers.14.final_layer_norm.bias', 'encoder.layers.17.attention.v_proj.weight', 'encoder.layers.15.attention.v_proj.weight', 'encoder.layers.20.attention.q_proj.weight', 'encoder.layers.5.attention.v_proj.bias', 'feature_extractor.conv_layers.4.layer_norm.weight', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.7.attention.q_proj.weight', 'encoder.layers.6.attention.q_proj.weight', 'feature_extractor.conv_layers.4.layer_norm.bias', 'encoder.layers.0.attention.k_proj.weight', 'encoder.layers.16.attention.k_proj.weight', 'encoder.layers.22.attention.v_proj.weight', 'encoder.layers.13.attention.out_proj.weight', 'encoder.layers.8.attention.q_proj.weight', 'encoder.layers.12.attention.q_proj.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.6.feed_forward.output_dense.bias', 'encoder.layers.2.attention.k_proj.weight', 'classifier.dense.weight', 'encoder.layers.17.attention.q_proj.weight', 'encoder.layers.15.feed_forward.intermediate_dense.weight', 'encoder.layers.22.attention.out_proj.weight', 'encoder.layers.12.feed_forward.output_dense.weight', 'encoder.layers.3.attention.q_proj.weight', 'feature_extractor.conv_layers.6.conv.weight', 'encoder.layers.21.final_layer_norm.bias', 'encoder.layers.1.attention.k_proj.bias', 'encoder.layers.10.attention.v_proj.bias', 'encoder.layers.22.feed_forward.output_dense.weight', 'encoder.layers.13.final_layer_norm.weight', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.10.attention.out_proj.bias', 'encoder.layers.16.feed_forward.intermediate_dense.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.3.attention.q_proj.bias', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.21.attention.k_proj.weight', 'encoder.layers.16.attention.out_proj.bias', 'encoder.layers.23.attention.q_proj.bias', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.3.attention.k_proj.weight', 'encoder.layers.13.feed_forward.output_dense.bias', 'encoder.layers.0.attention.q_proj.weight', 'encoder.layers.11.feed_forward.output_dense.bias', 'encoder.layers.12.attention.v_proj.bias', 'encoder.layers.22.feed_forward.intermediate_dense.bias', 'encoder.layers.3.feed_forward.intermediate_dense.weight', 'encoder.layers.19.attention.v_proj.bias', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.12.attention.out_proj.bias', 'encoder.layers.5.attention.k_proj.bias', 'encoder.layers.13.attention.k_proj.bias', 'encoder.layers.15.attention.q_proj.bias', 'encoder.layers.12.attention.out_proj.weight', 'encoder.layers.6.attention.k_proj.weight', 'feature_extractor.conv_layers.3.conv.weight', 'encoder.layers.23.attention.out_proj.bias', 'encoder.layers.14.feed_forward.intermediate_dense.weight', 'encoder.layers.0.attention.k_proj.bias', 'encoder.layers.15.feed_forward.output_dense.weight', 'encoder.layers.0.feed_forward.intermediate_dense.bias', 'encoder.layers.5.attention.k_proj.weight', 'encoder.layers.8.attention.v_proj.bias', 'encoder.layers.10.attention.out_proj.weight', 'encoder.layers.1.feed_forward.output_dense.bias', 'encoder.layers.16.attention.v_proj.weight', 'encoder.layers.3.attention.v_proj.weight', 'encoder.layers.23.attention.q_proj.weight', 'encoder.layers.3.feed_forward.output_dense.weight', 'encoder.layers.15.attention.q_proj.weight', 'encoder.layers.12.final_layer_norm.bias', 'encoder.layers.18.feed_forward.output_dense.bias', 'encoder.layers.6.feed_forward.output_dense.weight', 'encoder.layers.23.attention.out_proj.weight', 'encoder.layers.14.feed_forward.intermediate_dense.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.23.feed_forward.intermediate_dense.weight', 'encoder.layers.10.attention.v_proj.weight', 'encoder.layers.9.layer_norm.bias', 'encoder.layers.20.attention.k_proj.bias', 'encoder.layers.4.feed_forward.output_dense.bias', 'encoder.layers.2.feed_forward.output_dense.weight', 'encoder.layers.7.feed_forward.output_dense.weight', 'encoder.layers.6.attention.v_proj.weight', 'encoder.layers.20.final_layer_norm.weight', 'encoder.layers.16.attention.q_proj.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.19.final_layer_norm.bias', 'encoder.layers.21.layer_norm.bias', 'encoder.layers.9.attention.k_proj.weight', 'encoder.layers.14.feed_forward.output_dense.weight', 'encoder.layers.7.attention.k_proj.weight', 'encoder.layers.9.attention.out_proj.bias', 'encoder.layers.19.attention.q_proj.bias', 'encoder.layers.22.attention.k_proj.bias', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.14.attention.k_proj.bias', 'encoder.layers.19.attention.k_proj.weight', 'encoder.layers.2.feed_forward.output_dense.bias', 'encoder.layers.5.feed_forward.intermediate_dense.bias', 'feature_extractor.conv_layers.2.layer_norm.bias', 'encoder.layers.23.layer_norm.bias', 'encoder.layers.10.attention.q_proj.weight', 'encoder.layers.22.layer_norm.bias', 'encoder.layers.23.final_layer_norm.bias', 'encoder.layers.23.attention.v_proj.weight', 'encoder.layers.15.layer_norm.weight', 'encoder.layers.21.feed_forward.output_dense.bias', 'feature_extractor.conv_layers.1.conv.weight', 'encoder.layers.22.feed_forward.intermediate_dense.weight', 'encoder.layers.5.attention.v_proj.weight', 'encoder.layers.5.attention.q_proj.weight', 'encoder.layers.22.attention.q_proj.weight', 'encoder.layers.21.feed_forward.output_dense.weight', 'feature_extractor.conv_layers.5.layer_norm.weight', 'encoder.layers.8.feed_forward.intermediate_dense.bias', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.6.attention.out_proj.bias', 'encoder.layers.12.attention.k_proj.weight', 'encoder.layers.8.feed_forward.intermediate_dense.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.18.attention.k_proj.weight', 'encoder.layers.13.attention.k_proj.weight', 'encoder.layers.17.attention.v_proj.bias', 'feature_extractor.conv_layers.0.conv.bias', 'encoder.layers.4.attention.q_proj.weight', 'masked_spec_embed', 'encoder.layers.7.feed_forward.output_dense.bias', 'encoder.layers.19.feed_forward.output_dense.weight', 'encoder.layers.7.attention.out_proj.bias', 'encoder.layers.2.attention.q_proj.bias', 'encoder.layers.8.attention.out_proj.bias', 'encoder.layers.19.attention.v_proj.weight', 'encoder.layers.2.attention.q_proj.weight', 'encoder.layers.10.feed_forward.output_dense.weight', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.8.feed_forward.output_dense.bias', 'encoder.layers.7.attention.out_proj.weight', 'encoder.layers.4.feed_forward.intermediate_dense.bias', 'encoder.layers.5.attention.q_proj.bias', 'encoder.layers.18.feed_forward.intermediate_dense.bias', 'encoder.layers.1.attention.q_proj.bias', 'encoder.layers.3.feed_forward.intermediate_dense.bias', 'encoder.layers.10.attention.k_proj.weight', 'encoder.layers.13.attention.out_proj.bias', 'encoder.layers.0.feed_forward.intermediate_dense.weight', 'encoder.layers.11.attention.out_proj.bias', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.6.attention.out_proj.weight', 'encoder.layers.7.final_layer_norm.weight', 'feature_extractor.conv_layers.1.layer_norm.bias', 'encoder.layers.0.attention.q_proj.bias', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.12.attention.v_proj.weight', 'encoder.layers.16.final_layer_norm.weight', 'encoder.layers.5.attention.out_proj.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.4.feed_forward.output_dense.weight', 'encoder.layers.21.attention.k_proj.bias', 'encoder.layers.2.attention.k_proj.bias', 'encoder.layers.19.attention.q_proj.weight', 'encoder.layers.23.feed_forward.output_dense.bias', 'encoder.layers.20.final_layer_norm.bias', 'feature_extractor.conv_layers.4.conv.bias', 'encoder.layers.13.feed_forward.output_dense.weight', 'encoder.layers.13.layer_norm.bias', 'encoder.layers.8.attention.k_proj.weight', 'encoder.layers.16.feed_forward.output_dense.bias', 'encoder.layers.11.attention.k_proj.weight', 'feature_extractor.conv_layers.4.conv.weight', 'encoder.layers.18.attention.q_proj.bias', 'encoder.layers.6.attention.v_proj.bias', 'encoder.layers.12.layer_norm.bias', 'encoder.layers.15.final_layer_norm.bias', 'encoder.layers.21.attention.out_proj.weight', 'encoder.layers.1.attention.q_proj.weight', 'encoder.layers.14.attention.v_proj.weight', 'feature_extractor.conv_layers.2.layer_norm.weight', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.3.attention.out_proj.bias', 'encoder.layers.1.attention.k_proj.weight', 'encoder.layers.1.feed_forward.intermediate_dense.weight', 'encoder.layers.4.attention.out_proj.bias', 'feature_extractor.conv_layers.5.conv.weight', 'encoder.layers.21.feed_forward.intermediate_dense.weight', 'encoder.layers.22.feed_forward.output_dense.bias', 'encoder.layers.23.feed_forward.intermediate_dense.bias', 'encoder.layers.18.attention.v_proj.bias', 'encoder.layer_norm.bias', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.18.layer_norm.bias', 'feature_extractor.conv_layers.5.conv.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.13.attention.v_proj.weight', 'encoder.layers.0.attention.v_proj.weight', 'feature_extractor.conv_layers.1.layer_norm.weight', 'encoder.layers.18.attention.out_proj.bias', 'encoder.layers.16.attention.q_proj.weight', 'encoder.layers.6.layer_norm.bias', 'feature_projection.projection.bias', 'encoder.layers.7.layer_norm.weight', 'encoder.layers.17.feed_forward.intermediate_dense.bias', 'encoder.layer_norm.weight', 'encoder.layers.2.attention.v_proj.bias', 'encoder.layers.10.attention.q_proj.bias', 'feature_extractor.conv_layers.1.conv.bias', 'encoder.layers.14.layer_norm.bias', 'encoder.layers.10.layer_norm.weight', 'encoder.layers.5.feed_forward.output_dense.bias', 'encoder.layers.16.final_layer_norm.bias', 'feature_extractor.conv_layers.5.layer_norm.bias', 'encoder.layers.4.attention.q_proj.bias', 'encoder.layers.14.feed_forward.output_dense.bias', 'encoder.layers.17.layer_norm.bias', 'encoder.layers.4.attention.k_proj.weight', 'encoder.layers.12.final_layer_norm.weight', 'encoder.layers.5.feed_forward.intermediate_dense.weight', 'encoder.layers.13.attention.q_proj.bias', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.21.feed_forward.intermediate_dense.bias', 'encoder.layers.1.attention.v_proj.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.17.attention.k_proj.bias', 'encoder.layers.2.attention.v_proj.weight', 'encoder.layers.23.feed_forward.output_dense.weight', 'encoder.layers.10.feed_forward.output_dense.bias', 'feature_extractor.conv_layers.3.layer_norm.bias', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.14.attention.q_proj.weight', 'encoder.layers.15.attention.k_proj.weight', 'encoder.layers.18.feed_forward.output_dense.weight', 'encoder.pos_conv_embed.conv.weight_g', 'encoder.layers.2.feed_forward.intermediate_dense.weight', 'encoder.layers.14.attention.k_proj.weight', 'encoder.layers.13.layer_norm.weight', 'encoder.layers.10.attention.k_proj.bias', 'encoder.layers.19.feed_forward.intermediate_dense.weight', 'encoder.layers.20.feed_forward.output_dense.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.23.attention.k_proj.weight', 'encoder.layers.0.attention.out_proj.bias', 'encoder.layers.11.attention.q_proj.weight', 'encoder.layers.20.layer_norm.bias', 'encoder.layers.20.attention.v_proj.bias', 'encoder.layers.8.attention.k_proj.bias', 'encoder.layers.11.attention.out_proj.weight', 'encoder.layers.16.feed_forward.output_dense.weight', 'encoder.layers.0.attention.out_proj.weight', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.17.attention.out_proj.bias', 'encoder.layers.1.feed_forward.intermediate_dense.bias', 'encoder.layers.18.attention.q_proj.weight', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.12.layer_norm.weight', 'encoder.layers.9.attention.out_proj.weight', 'encoder.layers.7.layer_norm.bias', 'encoder.layers.20.feed_forward.intermediate_dense.bias', 'encoder.layers.4.feed_forward.intermediate_dense.weight', 'encoder.layers.16.attention.v_proj.bias', 'encoder.pos_conv_embed.conv.weight_v', 'encoder.layers.22.final_layer_norm.weight', 'encoder.layers.5.attention.out_proj.weight', 'encoder.layers.22.attention.out_proj.bias', 'encoder.layers.22.attention.q_proj.bias', 'classifier.dense.bias', 'encoder.layers.8.attention.q_proj.bias', 'encoder.layers.12.feed_forward.output_dense.bias', 'encoder.layers.11.feed_forward.intermediate_dense.bias', 'encoder.layers.17.final_layer_norm.bias', 'encoder.layers.0.attention.v_proj.bias', 'encoder.layers.4.final_layer_norm.bias', 'feature_extractor.conv_layers.0.conv.weight', 'encoder.layers.6.attention.q_proj.bias', 'encoder.layers.18.attention.out_proj.weight', 'encoder.layers.15.final_layer_norm.weight', 'encoder.layers.7.feed_forward.intermediate_dense.weight', 'encoder.layers.22.attention.k_proj.weight', 'encoder.layers.18.attention.v_proj.weight', 'encoder.layers.4.attention.out_proj.weight', 'encoder.layers.8.feed_forward.output_dense.weight', 'encoder.layers.21.attention.v_proj.bias', 'encoder.layers.20.layer_norm.weight', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.1.attention.out_proj.weight', 'encoder.layers.11.feed_forward.intermediate_dense.weight', 'encoder.layers.15.layer_norm.bias', 'encoder.layers.4.attention.v_proj.weight', 'encoder.layers.1.feed_forward.output_dense.weight', 'encoder.layers.4.attention.v_proj.bias', 'encoder.layers.21.attention.v_proj.weight', 'encoder.layers.13.attention.v_proj.bias', 'encoder.layers.17.final_layer_norm.weight', 'feature_projection.projection.weight', 'encoder.layers.23.attention.k_proj.bias', 'encoder.layers.19.attention.out_proj.bias', 'encoder.layers.9.feed_forward.output_dense.bias', 'encoder.layers.10.feed_forward.intermediate_dense.weight', 'encoder.layers.21.attention.q_proj.bias', 'feature_extractor.conv_layers.6.layer_norm.weight', 'encoder.layers.18.layer_norm.weight', 'classifier.out_proj.weight', 'encoder.layers.11.layer_norm.bias', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.22.attention.v_proj.bias', 'encoder.layers.19.attention.out_proj.weight', 'encoder.layers.12.feed_forward.intermediate_dense.weight', 'encoder.layers.1.attention.v_proj.weight', 'encoder.layers.8.attention.out_proj.weight', 'encoder.layers.18.feed_forward.intermediate_dense.weight', 'encoder.layers.21.attention.out_proj.bias', 'encoder.layers.14.layer_norm.weight', 'encoder.layers.10.layer_norm.bias', 'encoder.layers.2.attention.out_proj.weight', 'encoder.layers.9.attention.v_proj.weight', 'encoder.layers.10.feed_forward.intermediate_dense.bias', 'encoder.layers.11.attention.k_proj.bias', 'encoder.layers.15.attention.out_proj.bias', 'encoder.layers.17.feed_forward.intermediate_dense.weight', 'encoder.layers.15.attention.k_proj.bias', 'encoder.layers.17.attention.k_proj.weight', 'encoder.layers.16.feed_forward.intermediate_dense.weight', 'feature_extractor.conv_layers.2.conv.weight', 'encoder.layers.20.attention.k_proj.weight', 'encoder.layers.11.attention.v_proj.bias', 'encoder.layers.12.attention.q_proj.weight', 'encoder.layers.9.attention.q_proj.weight', 'encoder.layers.20.attention.v_proj.weight', 'encoder.layers.7.attention.k_proj.bias', 'encoder.layers.15.feed_forward.intermediate_dense.bias', 'encoder.layers.22.final_layer_norm.bias', 'encoder.layers.13.final_layer_norm.bias', 'feature_extractor.conv_layers.3.layer_norm.weight', 'encoder.layers.2.feed_forward.intermediate_dense.bias', 'encoder.layers.7.attention.q_proj.bias', 'encoder.layers.7.attention.v_proj.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.20.attention.q_proj.bias', 'encoder.layers.15.attention.out_proj.weight', 'encoder.layers.20.feed_forward.output_dense.weight', 'encoder.pos_conv_embed.conv.bias', 'encoder.layers.14.attention.v_proj.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.13.feed_forward.intermediate_dense.bias', 'encoder.layers.16.layer_norm.weight', 'encoder.layers.8.layer_norm.bias', 'encoder.layers.14.attention.q_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: valence_annotation_phase, target, split, gender, expression, speaker_id, manual_transcription, arousal_annotation_phase, path, Unnamed: 0, utterance_id, discrete_emotion_collection_phase, age, discrete_emotion_annotation_phase. If valence_annotation_phase, target, split, gender, expression, speaker_id, manual_transcription, arousal_annotation_phase, path, Unnamed: 0, utterance_id, discrete_emotion_collection_phase, age, discrete_emotion_annotation_phase are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 702\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1750\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m5roop\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/peterr/macocu/task14/wandb/run-20220906_110453-3ce43k6u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/5roop/huggingface/runs/3ce43k6u\" target=\"_blank\">models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_</a></strong> to <a href=\"https://wandb.ai/5roop/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='876' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 876/1750 07:42 < 07:42, 1.89 it/s, Epoch 5.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.714900</td>\n",
       "      <td>1.793728</td>\n",
       "      <td>0.287129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.573500</td>\n",
       "      <td>1.652131</td>\n",
       "      <td>0.178218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.655200</td>\n",
       "      <td>1.626233</td>\n",
       "      <td>0.178218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.795300</td>\n",
       "      <td>1.602993</td>\n",
       "      <td>0.287129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.606300</td>\n",
       "      <td>1.589786</td>\n",
       "      <td>0.287129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: valence_annotation_phase, target, split, gender, expression, speaker_id, manual_transcription, arousal_annotation_phase, path, Unnamed: 0, utterance_id, discrete_emotion_collection_phase, age, discrete_emotion_annotation_phase. If valence_annotation_phase, target, split, gender, expression, speaker_id, manual_transcription, arousal_annotation_phase, path, Unnamed: 0, utterance_id, discrete_emotion_collection_phase, age, discrete_emotion_annotation_phase are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 202\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-175\n",
      "Configuration saved in models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-175/config.json\n",
      "Model weights saved in models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-175/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-175/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: valence_annotation_phase, target, split, gender, expression, speaker_id, manual_transcription, arousal_annotation_phase, path, Unnamed: 0, utterance_id, discrete_emotion_collection_phase, age, discrete_emotion_annotation_phase. If valence_annotation_phase, target, split, gender, expression, speaker_id, manual_transcription, arousal_annotation_phase, path, Unnamed: 0, utterance_id, discrete_emotion_collection_phase, age, discrete_emotion_annotation_phase are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 202\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-350\n",
      "Configuration saved in models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-350/config.json\n",
      "Model weights saved in models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-350/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-350/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-175] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: valence_annotation_phase, target, split, gender, expression, speaker_id, manual_transcription, arousal_annotation_phase, path, Unnamed: 0, utterance_id, discrete_emotion_collection_phase, age, discrete_emotion_annotation_phase. If valence_annotation_phase, target, split, gender, expression, speaker_id, manual_transcription, arousal_annotation_phase, path, Unnamed: 0, utterance_id, discrete_emotion_collection_phase, age, discrete_emotion_annotation_phase are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 202\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-525\n",
      "Configuration saved in models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-525/config.json\n",
      "Model weights saved in models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-525/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-525/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-350] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: valence_annotation_phase, target, split, gender, expression, speaker_id, manual_transcription, arousal_annotation_phase, path, Unnamed: 0, utterance_id, discrete_emotion_collection_phase, age, discrete_emotion_annotation_phase. If valence_annotation_phase, target, split, gender, expression, speaker_id, manual_transcription, arousal_annotation_phase, path, Unnamed: 0, utterance_id, discrete_emotion_collection_phase, age, discrete_emotion_annotation_phase are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 202\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-700\n",
      "Configuration saved in models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-700/config.json\n",
      "Model weights saved in models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-700/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-700/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-525] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: valence_annotation_phase, target, split, gender, expression, speaker_id, manual_transcription, arousal_annotation_phase, path, Unnamed: 0, utterance_id, discrete_emotion_collection_phase, age, discrete_emotion_annotation_phase. If valence_annotation_phase, target, split, gender, expression, speaker_id, manual_transcription, arousal_annotation_phase, path, Unnamed: 0, utterance_id, discrete_emotion_collection_phase, age, discrete_emotion_annotation_phase are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 202\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-875\n",
      "Configuration saved in models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-875/config.json\n",
      "Model weights saved in models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-875/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_hubert-large-ls960-ft_emotion_optimal_epochs_/checkpoint-875/preprocessor_config.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "    \n",
    "checkpoints = [\n",
    "    # \"facebook/wav2vec2-large-slavic-voxpopuli-v2\",\n",
    "    # \"facebook/wav2vec2-large-960h-lv60-self\",\n",
    "    # \"classla/wav2vec2-large-slavic-parlaspeech-hr\",\n",
    "    \"facebook/hubert-large-ls960-ft\",\n",
    "]\n",
    "optimal_epochs = {\n",
    "    \"facebook/wav2vec2-large-960h-lv60-self\": 9,\n",
    "    \"facebook/wav2vec2-large-slavic-voxpopuli-v2\": 11,\n",
    "    \"classla/wav2vec2-large-slavic-parlaspeech-hr\": 7,  \n",
    "    \"facebook/hubert-large-ls960-ft\": 10,\n",
    "}\n",
    "from utils import train_model, eval_model\n",
    "for checkpoint in checkpoints:\n",
    "    import os\n",
    "    os.system(\"rm -r models/*\")\n",
    "    train_config = dict(\n",
    "        model_name_or_path = checkpoint,\n",
    "        TASK = \"emotion_optimal_epochs\",\n",
    "        NUM_EPOCH = optimal_epochs.get(checkpoint),\n",
    "        output_column = \"target\",\n",
    "        input_column = \"path\",\n",
    "        data_files = {\n",
    "            \"train\": \"007_train.csv\",\n",
    "            \"validation\": \"007_dev.csv\",\n",
    "        },\n",
    "        clip_seconds = 10,\n",
    "    )\n",
    "\n",
    "    output_dir = train_model(train_config)\n",
    "    import numpy as np\n",
    "    for split in \"dev test\".split():\n",
    "        results = []\n",
    "        from pathlib import Path\n",
    "        found_path = str(list(Path(output_dir).glob(\"checkpoint-*\"))[0])\n",
    "        print(f\"For checkpoint: {checkpoint} a path was found: \", found_path)\n",
    "        eval_config = dict(\n",
    "            output_column = \"target\",\n",
    "            model_name_or_path= found_path ,\n",
    "            eval_file= f\"007_{split}.csv\"\n",
    "        )\n",
    "\n",
    "        y_true, y_pred = eval_model(eval_config)\n",
    "        import datetime\n",
    "        time = str(datetime.datetime.now())\n",
    "        results.append(\n",
    "            {\n",
    "                **eval_config, \n",
    "                **train_config,\n",
    "                \"split\": split,\n",
    "                \"y_true\": y_true, \n",
    "                \"y_pred\": y_pred,\n",
    "                \"time\": time,\n",
    "                        })\n",
    "        content = pd.DataFrame(data=results).to_json(None, \n",
    "                                           orient=\"records\",\n",
    "                                           lines=True,\n",
    "                                           )\n",
    "        with open(\"008_results_new_new_new.jsonl\", \"a\") as f:\n",
    "            f.writelines(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f6f5766036ee03d059e365a942add07f79c17033585e9357ee8157d52fe6bb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
