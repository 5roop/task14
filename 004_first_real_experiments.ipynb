{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'models/*': No such file or directory\n",
      "Using custom data configuration default-ca47a7a6555e75ab\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-ca47a7a6555e75ab/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a62190c345e413db5d47fa907f55410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A classification problem with 5 classes: ['anger', 'fear', 'happiness', 'neutral', 'sadness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-ca47a7a6555e75ab/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-f97d64a3dddfed2e.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-ca47a7a6555e75ab/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-79de85513ed48579.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-ca47a7a6555e75ab/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-c16e7d1d17696476.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-ca47a7a6555e75ab/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-5cedf9ae61f019bf.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-ca47a7a6555e75ab/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-bd98faad7fc64945.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-ca47a7a6555e75ab/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-04bffa8fdd1ef21c.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-ca47a7a6555e75ab/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-9f14ffb06971d231.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-ca47a7a6555e75ab/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-b29f5577feb52237.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target sampling rate: 16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-large-960h-lv60-self were not used when initializing Wav2Vec2ForSpeechClassification: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'wav2vec2.masked_spec_embed', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: manual_transcription, discrete_emotion_collection_phase, gender, path, arousal_annotation_phase, age, target, utterance_id, expression, speaker_id, valence_annotation_phase, discrete_emotion_annotation_phase, Unnamed: 0, split. If manual_transcription, discrete_emotion_collection_phase, gender, path, arousal_annotation_phase, age, target, utterance_id, expression, speaker_id, valence_annotation_phase, discrete_emotion_annotation_phase, Unnamed: 0, split are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 704\n",
      "  Num Epochs = 9\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1584\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m5roop\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/peterr/macocu/task14/wandb/run-20220830_103344-2vrv4vcs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/5roop/huggingface/runs/2vrv4vcs\" target=\"_blank\">models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_</a></strong> to <a href=\"https://wandb.ai/5roop/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1584' max='1584' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1584/1584 14:23, Epoch 9/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.461900</td>\n",
       "      <td>1.541887</td>\n",
       "      <td>0.371287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.947300</td>\n",
       "      <td>1.230440</td>\n",
       "      <td>0.509901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.164400</td>\n",
       "      <td>1.346554</td>\n",
       "      <td>0.539604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.237100</td>\n",
       "      <td>1.264827</td>\n",
       "      <td>0.544554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.128300</td>\n",
       "      <td>1.227096</td>\n",
       "      <td>0.628713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.595100</td>\n",
       "      <td>1.108959</td>\n",
       "      <td>0.653465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.524700</td>\n",
       "      <td>1.242364</td>\n",
       "      <td>0.688119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.299100</td>\n",
       "      <td>1.446237</td>\n",
       "      <td>0.673267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.611300</td>\n",
       "      <td>1.345919</td>\n",
       "      <td>0.698020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: manual_transcription, discrete_emotion_collection_phase, gender, path, arousal_annotation_phase, age, target, utterance_id, expression, speaker_id, valence_annotation_phase, discrete_emotion_annotation_phase, Unnamed: 0, split. If manual_transcription, discrete_emotion_collection_phase, gender, path, arousal_annotation_phase, age, target, utterance_id, expression, speaker_id, valence_annotation_phase, discrete_emotion_annotation_phase, Unnamed: 0, split are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 202\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-176\n",
      "Configuration saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-176/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-176/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-176/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: manual_transcription, discrete_emotion_collection_phase, gender, path, arousal_annotation_phase, age, target, utterance_id, expression, speaker_id, valence_annotation_phase, discrete_emotion_annotation_phase, Unnamed: 0, split. If manual_transcription, discrete_emotion_collection_phase, gender, path, arousal_annotation_phase, age, target, utterance_id, expression, speaker_id, valence_annotation_phase, discrete_emotion_annotation_phase, Unnamed: 0, split are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 202\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-352\n",
      "Configuration saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-352/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-352/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-352/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-176] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: manual_transcription, discrete_emotion_collection_phase, gender, path, arousal_annotation_phase, age, target, utterance_id, expression, speaker_id, valence_annotation_phase, discrete_emotion_annotation_phase, Unnamed: 0, split. If manual_transcription, discrete_emotion_collection_phase, gender, path, arousal_annotation_phase, age, target, utterance_id, expression, speaker_id, valence_annotation_phase, discrete_emotion_annotation_phase, Unnamed: 0, split are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 202\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-528\n",
      "Configuration saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-528/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-528/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-528/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-352] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: manual_transcription, discrete_emotion_collection_phase, gender, path, arousal_annotation_phase, age, target, utterance_id, expression, speaker_id, valence_annotation_phase, discrete_emotion_annotation_phase, Unnamed: 0, split. If manual_transcription, discrete_emotion_collection_phase, gender, path, arousal_annotation_phase, age, target, utterance_id, expression, speaker_id, valence_annotation_phase, discrete_emotion_annotation_phase, Unnamed: 0, split are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 202\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-704\n",
      "Configuration saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-704/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-704/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-704/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-528] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: manual_transcription, discrete_emotion_collection_phase, gender, path, arousal_annotation_phase, age, target, utterance_id, expression, speaker_id, valence_annotation_phase, discrete_emotion_annotation_phase, Unnamed: 0, split. If manual_transcription, discrete_emotion_collection_phase, gender, path, arousal_annotation_phase, age, target, utterance_id, expression, speaker_id, valence_annotation_phase, discrete_emotion_annotation_phase, Unnamed: 0, split are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 202\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-880\n",
      "Configuration saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-880/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-880/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-880/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-704] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: manual_transcription, discrete_emotion_collection_phase, gender, path, arousal_annotation_phase, age, target, utterance_id, expression, speaker_id, valence_annotation_phase, discrete_emotion_annotation_phase, Unnamed: 0, split. If manual_transcription, discrete_emotion_collection_phase, gender, path, arousal_annotation_phase, age, target, utterance_id, expression, speaker_id, valence_annotation_phase, discrete_emotion_annotation_phase, Unnamed: 0, split are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 202\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1056\n",
      "Configuration saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1056/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1056/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1056/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-880] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: manual_transcription, discrete_emotion_collection_phase, gender, path, arousal_annotation_phase, age, target, utterance_id, expression, speaker_id, valence_annotation_phase, discrete_emotion_annotation_phase, Unnamed: 0, split. If manual_transcription, discrete_emotion_collection_phase, gender, path, arousal_annotation_phase, age, target, utterance_id, expression, speaker_id, valence_annotation_phase, discrete_emotion_annotation_phase, Unnamed: 0, split are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 202\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1232\n",
      "Configuration saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1232/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1232/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1232/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1056] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: manual_transcription, discrete_emotion_collection_phase, gender, path, arousal_annotation_phase, age, target, utterance_id, expression, speaker_id, valence_annotation_phase, discrete_emotion_annotation_phase, Unnamed: 0, split. If manual_transcription, discrete_emotion_collection_phase, gender, path, arousal_annotation_phase, age, target, utterance_id, expression, speaker_id, valence_annotation_phase, discrete_emotion_annotation_phase, Unnamed: 0, split are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 202\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1408\n",
      "Configuration saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1408/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1408/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1408/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1232] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: manual_transcription, discrete_emotion_collection_phase, gender, path, arousal_annotation_phase, age, target, utterance_id, expression, speaker_id, valence_annotation_phase, discrete_emotion_annotation_phase, Unnamed: 0, split. If manual_transcription, discrete_emotion_collection_phase, gender, path, arousal_annotation_phase, age, target, utterance_id, expression, speaker_id, valence_annotation_phase, discrete_emotion_annotation_phase, Unnamed: 0, split are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 202\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584\n",
      "Configuration saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1408] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For checkpoint: facebook/wav2vec2-large-960h-lv60-self a path was found:  models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-f70a6d955a732867\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-f70a6d955a732867/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c758f7ece8e0450fb394372b87646dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForSpeechClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"finetuning_task\": \"wav2vec2_clf\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"anger\",\n",
      "    \"1\": \"fear\",\n",
      "    \"2\": \"happiness\",\n",
      "    \"3\": \"neutral\",\n",
      "    \"4\": \"sadness\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 0,\n",
      "    \"fear\": 1,\n",
      "    \"happiness\": 2,\n",
      "    \"neutral\": 3,\n",
      "    \"sadness\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooling_mode\": \"mean\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading feature extractor configuration file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/preprocessor_config.json\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": true,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForSpeechClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"finetuning_task\": \"wav2vec2_clf\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"anger\",\n",
      "    \"1\": \"fear\",\n",
      "    \"2\": \"happiness\",\n",
      "    \"3\": \"neutral\",\n",
      "    \"4\": \"sadness\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 0,\n",
      "    \"fear\": 1,\n",
      "    \"happiness\": 2,\n",
      "    \"neutral\": 3,\n",
      "    \"sadness\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooling_mode\": \"mean\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "Didn't find file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/vocab.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/tokenizer_config.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/added_tokens.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/special_tokens_map.json. We won't load it.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:52: FutureWarning: Loading a tokenizer inside Wav2Vec2Processor from a config that does not include a `tokenizer_class` attribute is deprecated and will be removed in v5. Please add `'tokenizer_class': 'Wav2Vec2CTCTokenizer'` attribute to either your `config.json` or `tokenizer_config.json` file to suppress this warning: \n",
      "  warnings.warn(\n",
      "loading feature extractor configuration file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/preprocessor_config.json\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": true,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "Didn't find file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/vocab.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/tokenizer_config.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/added_tokens.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./tokenizer_config.json. We won't load it.\n",
      "Didn't find file ./added_tokens.json. We won't load it.\n",
      "Didn't find file ./special_tokens_map.json. We won't load it.\n",
      "loading file ./vocab.json\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "Adding <s> to the vocabulary\n",
      "Adding </s> to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-large-960h-lv60-self\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForSpeechClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"finetuning_task\": \"wav2vec2_clf\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"anger\",\n",
      "    \"1\": \"fear\",\n",
      "    \"2\": \"happiness\",\n",
      "    \"3\": \"neutral\",\n",
      "    \"4\": \"sadness\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 0,\n",
      "    \"fear\": 1,\n",
      "    \"happiness\": 2,\n",
      "    \"neutral\": 3,\n",
      "    \"sadness\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooling_mode\": \"mean\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing Wav2Vec2ForSpeechClassification.\n",
      "\n",
      "All the weights of Wav2Vec2ForSpeechClassification were initialized from the model checkpoint at models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Wav2Vec2ForSpeechClassification for predictions without further training.\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-f70a6d955a732867/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-c7ce7e560767d834.arrow\n",
      "Parameter 'function'=<function eval_model.<locals>.predict at 0x7ff593d3e700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-f70a6d955a732867/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-0ef152125425b7b2.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For checkpoint: facebook/wav2vec2-large-960h-lv60-self a path was found:  models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-38cc960bf51fe830\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-38cc960bf51fe830/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0786fe48c33c47198bb85500c1f3f710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForSpeechClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"finetuning_task\": \"wav2vec2_clf\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"anger\",\n",
      "    \"1\": \"fear\",\n",
      "    \"2\": \"happiness\",\n",
      "    \"3\": \"neutral\",\n",
      "    \"4\": \"sadness\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 0,\n",
      "    \"fear\": 1,\n",
      "    \"happiness\": 2,\n",
      "    \"neutral\": 3,\n",
      "    \"sadness\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooling_mode\": \"mean\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading feature extractor configuration file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/preprocessor_config.json\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": true,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForSpeechClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"finetuning_task\": \"wav2vec2_clf\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"anger\",\n",
      "    \"1\": \"fear\",\n",
      "    \"2\": \"happiness\",\n",
      "    \"3\": \"neutral\",\n",
      "    \"4\": \"sadness\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 0,\n",
      "    \"fear\": 1,\n",
      "    \"happiness\": 2,\n",
      "    \"neutral\": 3,\n",
      "    \"sadness\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooling_mode\": \"mean\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "Didn't find file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/vocab.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/tokenizer_config.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/added_tokens.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/special_tokens_map.json. We won't load it.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:52: FutureWarning: Loading a tokenizer inside Wav2Vec2Processor from a config that does not include a `tokenizer_class` attribute is deprecated and will be removed in v5. Please add `'tokenizer_class': 'Wav2Vec2CTCTokenizer'` attribute to either your `config.json` or `tokenizer_config.json` file to suppress this warning: \n",
      "  warnings.warn(\n",
      "loading feature extractor configuration file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/preprocessor_config.json\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": true,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "Didn't find file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/vocab.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/tokenizer_config.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/added_tokens.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./tokenizer_config.json. We won't load it.\n",
      "Didn't find file ./added_tokens.json. We won't load it.\n",
      "Didn't find file ./special_tokens_map.json. We won't load it.\n",
      "loading file ./vocab.json\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "Adding <s> to the vocabulary\n",
      "Adding </s> to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-large-960h-lv60-self\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForSpeechClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"finetuning_task\": \"wav2vec2_clf\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"anger\",\n",
      "    \"1\": \"fear\",\n",
      "    \"2\": \"happiness\",\n",
      "    \"3\": \"neutral\",\n",
      "    \"4\": \"sadness\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 0,\n",
      "    \"fear\": 1,\n",
      "    \"happiness\": 2,\n",
      "    \"neutral\": 3,\n",
      "    \"sadness\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooling_mode\": \"mean\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing Wav2Vec2ForSpeechClassification.\n",
      "\n",
      "All the weights of Wav2Vec2ForSpeechClassification were initialized from the model checkpoint at models/facebook_wav2vec2-large-960h-lv60-self_emotion_optimal_epochs_/checkpoint-1584.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Wav2Vec2ForSpeechClassification for predictions without further training.\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-38cc960bf51fe830/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-6e19cfb6019f2fd1.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-38cc960bf51fe830/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-055665f0fbb3e84e.arrow\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def purge():\n",
    "    import torch\n",
    "    from numba import cuda\n",
    "    import gc\n",
    "    from time import sleep\n",
    "    try:\n",
    "        cuda.select_device(0)\n",
    "        cuda.close()\n",
    "        cuda.select_device(0)\n",
    "        gc.collect()\n",
    "        sleep(2)\n",
    "        torch.cuda.empty_cache()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "checkpoints = [\n",
    "    # \"facebook/wav2vec2-large-slavic-voxpopuli-v2\",\n",
    "    \"facebook/wav2vec2-large-960h-lv60-self\",\n",
    "    # \"classla/wav2vec2-large-slavic-parlaspeech-hr\",\n",
    "]\n",
    "optimal_epochs = {\n",
    "    \"facebook/wav2vec2-large-960h-lv60-self\": 9,\n",
    "    \"facebook/wav2vec2-large-slavic-voxpopuli-v2\": 11,\n",
    "    \"classla/wav2vec2-large-slavic-parlaspeech-hr\": 7,  \n",
    "}\n",
    "from utils import train_model, eval_model\n",
    "results = []\n",
    "for checkpoint in checkpoints:\n",
    "    import os\n",
    "    os.system(\"rm -r models/*\")\n",
    "    train_config = dict(\n",
    "        model_name_or_path = checkpoint,\n",
    "        TASK = \"emotion_optimal_epochs\",\n",
    "        NUM_EPOCH = optimal_epochs.get(checkpoint),\n",
    "        output_column = \"target\",\n",
    "        input_column = \"path\",\n",
    "        data_files = {\n",
    "            \"train\": \"001_train.csv\",\n",
    "            \"validation\": \"001_dev.csv\",\n",
    "        },\n",
    "        clip_seconds = 10,\n",
    "    )\n",
    "\n",
    "    output_dir = train_model(train_config)\n",
    "    import numpy as np\n",
    "    for split in \"dev test\".split():\n",
    "        from pathlib import Path\n",
    "        found_path = str(list(Path(output_dir).glob(\"checkpoint-*\"))[0])\n",
    "        print(f\"For checkpoint: {checkpoint} a path was found: \", found_path)\n",
    "        eval_config = dict(\n",
    "            output_column = \"target\",\n",
    "            model_name_or_path= found_path ,\n",
    "            eval_file= f\"001_{split}.csv\"\n",
    "        )\n",
    "\n",
    "        y_true, y_pred = eval_model(eval_config)\n",
    "        results.append({**eval_config, \n",
    "                        **train_config,\n",
    "                        \"split\": split,\n",
    "                        \"y_true\": y_true, \n",
    "                        \"y_pred\": y_pred})\n",
    "        content = pd.DataFrame(data=results).to_json(None, \n",
    "                                           orient=\"records\",\n",
    "                                           lines=True,\n",
    "                                           )\n",
    "        with open(\"004_results_new_new.jsonl\", \"a\") as f:\n",
    "            f.writelines(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f6f5766036ee03d059e365a942add07f79c17033585e9357ee8157d52fe6bb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
