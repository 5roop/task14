{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ca47a7a6555e75ab\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-ca47a7a6555e75ab/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e102909c92b6400f9ec5f652ced3bcde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A classification problem with 5 classes: ['anger', 'fear', 'happiness', 'neutral', 'sadness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/classla/wav2vec2-large-slavic-parlaspeech-hr/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/05ba655a8826008bb02f30f10fa5690b3df89e2b74031e1c8f9ead2da53e7173.1df24b662a4164adc8330994f0ed99b3b44c58e2263205e3ddba275170f00570\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"classla/wav2vec2-large-slavic-parlaspeech-hr\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"mean\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.0,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"finetuning_task\": \"wav2vec2_clf\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"anger\",\n",
      "    \"1\": \"fear\",\n",
      "    \"2\": \"happiness\",\n",
      "    \"3\": \"neutral\",\n",
      "    \"4\": \"sadness\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 0,\n",
      "    \"fear\": 1,\n",
      "    \"happiness\": 2,\n",
      "    \"neutral\": 3,\n",
      "    \"sadness\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 50,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading feature extractor configuration file https://huggingface.co/classla/wav2vec2-large-slavic-parlaspeech-hr/resolve/main/preprocessor_config.json from cache at /home/peterr/.cache/huggingface/transformers/431b24489a6b00be892c07912f671f97a484cab9fffba31d6afa0443726b33f9.bbc1eb890a39c82e710a893223b8452ac5b78e8b57083b2f893aa7dc59d4ed69\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": true,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/classla/wav2vec2-large-slavic-parlaspeech-hr/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/05ba655a8826008bb02f30f10fa5690b3df89e2b74031e1c8f9ead2da53e7173.1df24b662a4164adc8330994f0ed99b3b44c58e2263205e3ddba275170f00570\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"classla/wav2vec2-large-slavic-parlaspeech-hr\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"mean\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.0,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 50,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/classla/wav2vec2-large-slavic-parlaspeech-hr/resolve/main/vocab.json from cache at /home/peterr/.cache/huggingface/transformers/cb577d9f30536ea0e5498ed1436c4817c85ebb21d8d636c9f148e2d582ea4e66.9fca1730dd4efcb55aaea3c92e0af1a0b8a568eb39093fda6b56e184342488b0\n",
      "loading file https://huggingface.co/classla/wav2vec2-large-slavic-parlaspeech-hr/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/classla/wav2vec2-large-slavic-parlaspeech-hr/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/classla/wav2vec2-large-slavic-parlaspeech-hr/resolve/main/special_tokens_map.json from cache at None\n",
      "loading configuration file https://huggingface.co/classla/wav2vec2-large-slavic-parlaspeech-hr/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/05ba655a8826008bb02f30f10fa5690b3df89e2b74031e1c8f9ead2da53e7173.1df24b662a4164adc8330994f0ed99b3b44c58e2263205e3ddba275170f00570\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"classla/wav2vec2-large-slavic-parlaspeech-hr\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"mean\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.0,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 50,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "Adding <s> to the vocabulary\n",
      "Adding </s> to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-ca47a7a6555e75ab/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-177d636106695dba.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-ca47a7a6555e75ab/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-13d7e946c120a06c.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-ca47a7a6555e75ab/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-9cf79c91170c9542.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-ca47a7a6555e75ab/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-872d30a27946fe65.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-ca47a7a6555e75ab/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-ef562a2ba83df1ea.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-ca47a7a6555e75ab/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-cd7eadfc3a32d502.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-ca47a7a6555e75ab/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-7131dfcbd496c176.arrow\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-ca47a7a6555e75ab/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-f31ae704bb2e4482.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target sampling rate: 16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/classla/wav2vec2-large-slavic-parlaspeech-hr/resolve/main/pytorch_model.bin from cache at /home/peterr/.cache/huggingface/transformers/5662ba6d35369662a60e23036a4256ad94fb28c06a295a6241e9f1511e10f93a.cadfc2d38d4fbbc4c3d3ef91b87fbe5b9cc2ce8432d754f9b42ffc0cffba07c4\n",
      "Some weights of the model checkpoint at classla/wav2vec2-large-slavic-parlaspeech-hr were not used when initializing Wav2Vec2ForSpeechClassification: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at classla/wav2vec2-large-slavic-parlaspeech-hr and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: discrete_emotion_annotation_phase, age, Unnamed: 0, expression, manual_transcription, arousal_annotation_phase, split, gender, utterance_id, speaker_id, path, target, discrete_emotion_collection_phase, valence_annotation_phase. If discrete_emotion_annotation_phase, age, Unnamed: 0, expression, manual_transcription, arousal_annotation_phase, split, gender, utterance_id, speaker_id, path, target, discrete_emotion_collection_phase, valence_annotation_phase are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 704\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1232\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1202' max='1232' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1202/1232 11:06 < 00:16, 1.80 it/s, Epoch 6.82/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.988500</td>\n",
       "      <td>0.953553</td>\n",
       "      <td>0.702970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.551300</td>\n",
       "      <td>1.074658</td>\n",
       "      <td>0.688119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.698500</td>\n",
       "      <td>1.417316</td>\n",
       "      <td>0.683168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>1.418077</td>\n",
       "      <td>0.752475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.025100</td>\n",
       "      <td>1.413316</td>\n",
       "      <td>0.732673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.509832</td>\n",
       "      <td>0.737624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: discrete_emotion_annotation_phase, age, Unnamed: 0, expression, manual_transcription, arousal_annotation_phase, split, gender, utterance_id, speaker_id, path, target, discrete_emotion_collection_phase, valence_annotation_phase. If discrete_emotion_annotation_phase, age, Unnamed: 0, expression, manual_transcription, arousal_annotation_phase, split, gender, utterance_id, speaker_id, path, target, discrete_emotion_collection_phase, valence_annotation_phase are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 202\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-176\n",
      "Configuration saved in models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-176/config.json\n",
      "Model weights saved in models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-176/pytorch_model.bin\n",
      "Feature extractor saved in models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-176/preprocessor_config.json\n",
      "Deleting older checkpoint [models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-1232] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: discrete_emotion_annotation_phase, age, Unnamed: 0, expression, manual_transcription, arousal_annotation_phase, split, gender, utterance_id, speaker_id, path, target, discrete_emotion_collection_phase, valence_annotation_phase. If discrete_emotion_annotation_phase, age, Unnamed: 0, expression, manual_transcription, arousal_annotation_phase, split, gender, utterance_id, speaker_id, path, target, discrete_emotion_collection_phase, valence_annotation_phase are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 202\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-352\n",
      "Configuration saved in models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-352/config.json\n",
      "Model weights saved in models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-352/pytorch_model.bin\n",
      "Feature extractor saved in models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-352/preprocessor_config.json\n",
      "Deleting older checkpoint [models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-176] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: discrete_emotion_annotation_phase, age, Unnamed: 0, expression, manual_transcription, arousal_annotation_phase, split, gender, utterance_id, speaker_id, path, target, discrete_emotion_collection_phase, valence_annotation_phase. If discrete_emotion_annotation_phase, age, Unnamed: 0, expression, manual_transcription, arousal_annotation_phase, split, gender, utterance_id, speaker_id, path, target, discrete_emotion_collection_phase, valence_annotation_phase are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 202\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-528\n",
      "Configuration saved in models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-528/config.json\n",
      "Model weights saved in models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-528/pytorch_model.bin\n",
      "Feature extractor saved in models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-528/preprocessor_config.json\n",
      "Deleting older checkpoint [models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-352] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: discrete_emotion_annotation_phase, age, Unnamed: 0, expression, manual_transcription, arousal_annotation_phase, split, gender, utterance_id, speaker_id, path, target, discrete_emotion_collection_phase, valence_annotation_phase. If discrete_emotion_annotation_phase, age, Unnamed: 0, expression, manual_transcription, arousal_annotation_phase, split, gender, utterance_id, speaker_id, path, target, discrete_emotion_collection_phase, valence_annotation_phase are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 202\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-704\n",
      "Configuration saved in models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-704/config.json\n",
      "Model weights saved in models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-704/pytorch_model.bin\n",
      "Feature extractor saved in models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-704/preprocessor_config.json\n",
      "Deleting older checkpoint [models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-528] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: discrete_emotion_annotation_phase, age, Unnamed: 0, expression, manual_transcription, arousal_annotation_phase, split, gender, utterance_id, speaker_id, path, target, discrete_emotion_collection_phase, valence_annotation_phase. If discrete_emotion_annotation_phase, age, Unnamed: 0, expression, manual_transcription, arousal_annotation_phase, split, gender, utterance_id, speaker_id, path, target, discrete_emotion_collection_phase, valence_annotation_phase are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 202\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-880\n",
      "Configuration saved in models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-880/config.json\n",
      "Model weights saved in models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-880/pytorch_model.bin\n",
      "Feature extractor saved in models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-880/preprocessor_config.json\n",
      "Deleting older checkpoint [models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-704] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: discrete_emotion_annotation_phase, age, Unnamed: 0, expression, manual_transcription, arousal_annotation_phase, split, gender, utterance_id, speaker_id, path, target, discrete_emotion_collection_phase, valence_annotation_phase. If discrete_emotion_annotation_phase, age, Unnamed: 0, expression, manual_transcription, arousal_annotation_phase, split, gender, utterance_id, speaker_id, path, target, discrete_emotion_collection_phase, valence_annotation_phase are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 202\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-1056\n",
      "Configuration saved in models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-1056/config.json\n",
      "Model weights saved in models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-1056/pytorch_model.bin\n",
      "Feature extractor saved in models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-1056/preprocessor_config.json\n",
      "Deleting older checkpoint [models/classla_wav2vec2-large-slavic-parlaspeech-hr_emotion_optimal_epochs_/checkpoint-880] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def purge():\n",
    "    import torch\n",
    "    from numba import cuda\n",
    "    import gc\n",
    "    from time import sleep\n",
    "    try:\n",
    "        cuda.select_device(0)\n",
    "        cuda.close()\n",
    "        cuda.select_device(0)\n",
    "        gc.collect()\n",
    "        sleep(2)\n",
    "        torch.cuda.empty_cache()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "checkpoints = [\n",
    "    # \"facebook/wav2vec2-large-slavic-voxpopuli-v2\",\n",
    "    # \"facebook/wav2vec2-large-960h-lv60-self\",\n",
    "    \"classla/wav2vec2-large-slavic-parlaspeech-hr\",\n",
    "]\n",
    "optimal_epochs = {\n",
    "    \"facebook/wav2vec2-large-960h-lv60-self\": 9,\n",
    "    \"facebook/wav2vec2-large-slavic-voxpopuli-v2\": 11,\n",
    "    \"classla/wav2vec2-large-slavic-parlaspeech-hr\": 7,  \n",
    "}\n",
    "from utils import train_model, eval_model\n",
    "results = []\n",
    "for checkpoint in checkpoints:\n",
    "    train_config = dict(\n",
    "        model_name_or_path = checkpoint,\n",
    "        TASK = \"emotion_optimal_epochs\",\n",
    "        NUM_EPOCH = optimal_epochs.get(checkpoint),\n",
    "        output_column = \"target\",\n",
    "        input_column = \"path\",\n",
    "        data_files = {\n",
    "            \"train\": \"001_train.csv\",\n",
    "            \"validation\": \"001_dev.csv\",\n",
    "        },\n",
    "        clip_seconds = 10,\n",
    "    )\n",
    "\n",
    "    output_dir = train_model(train_config)\n",
    "    import numpy as np\n",
    "    for split in \"dev test\".split():\n",
    "        from pathlib import Path\n",
    "        eval_config = dict(\n",
    "            output_column = \"target\",\n",
    "            model_name_or_path= str(list(Path(output_dir).glob(\"checkpoint-*\"))[0]) ,\n",
    "            eval_file= f\"001_{split}.csv\"\n",
    "        )\n",
    "\n",
    "        y_true, y_pred = eval_model(eval_config)\n",
    "        results.append({**eval_config, \n",
    "                        **train_config,\n",
    "                        \"split\": split,\n",
    "                        \"y_true\": y_true, \n",
    "                        \"y_pred\": y_pred})\n",
    "        content = pd.DataFrame(data=results).to_json(None, \n",
    "                                           orient=\"records\",\n",
    "                                           lines=True,\n",
    "                                           )\n",
    "        with open(\"004_results_new.jsonl\", \"a\") as f:\n",
    "            f.writelines(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f6f5766036ee03d059e365a942add07f79c17033585e9357ee8157d52fe6bb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
